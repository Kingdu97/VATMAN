{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "NEAR_INF = 1e20\n",
    "NEAR_INF_FP16 = 65504\n",
    "\n",
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def save(toBeSaved, filename, mode='wb'):\n",
    "    '''\n",
    "    save data to pickle file\n",
    "    '''\n",
    "    dirname = os.path.dirname(filename)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    file = open(filename, mode)\n",
    "    pickle.dump(toBeSaved, file, protocol=4) # protocol 4 allows large size object, it's the default since python 3.8\n",
    "    file.close()\n",
    "\n",
    "def load(filename, mode='rb'):\n",
    "    '''\n",
    "    load pickle file\n",
    "    '''\n",
    "    file = open(filename, mode)\n",
    "    loaded = pickle.load(file)\n",
    "    file.close()\n",
    "    return loaded\n",
    "\n",
    "def pad_sents(sents, pad_token=0, max_len=512):\n",
    "    '''\n",
    "    pad input to max length\n",
    "    '''\n",
    "    sents_padded = []\n",
    "    lens = get_lens(sents)\n",
    "    max_len = min(max(lens), max_len)\n",
    "    sents_padded = []\n",
    "    new_len = []\n",
    "    for i, l in enumerate(lens):\n",
    "        if l > max_len:\n",
    "            l = max_len\n",
    "        new_len.append(l)\n",
    "        sents_padded.append(sents[i][:l] + [pad_token] * (max_len - l))\n",
    "    return sents_padded, new_len\n",
    "\n",
    "def get_mask(sents, unmask_idx=1, mask_idx=0, max_len=512):\n",
    "    '''\n",
    "    make mask for padded input\n",
    "    '''\n",
    "    lens = get_lens(sents)\n",
    "    max_len = min(max(lens), max_len)\n",
    "    mask = []\n",
    "    for l in lens:\n",
    "        if l > max_len:\n",
    "            l = max_len\n",
    "        mask.append([unmask_idx] * l + [mask_idx] * (max_len - l))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_lens(sents):\n",
    "    return [len(sent) for sent in sents]\n",
    "\n",
    "def get_max_len(sents):\n",
    "    max_len = max([len(sent) for sent in sents])\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "# from utils.utils import pad_sents, get_mask\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    \"\"\"Summarization dataset\"\"\"\n",
    "    def __init__(self, args, mode):\n",
    "        self.args = args\n",
    "        # initial tokenizer and text\n",
    "        if 't5' in self.args.model:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "        else:\n",
    "            self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "        if mode == 'train':\n",
    "            src_path = args.train_src_path\n",
    "            tgt_path = args.train_tgt_path\n",
    "        if mode == 'val':\n",
    "            src_path = args.val_src_path\n",
    "            tgt_path = args.val_tgt_path\n",
    "        if mode == 'test':\n",
    "            src_path = args.test_src_path\n",
    "            tgt_path = args.test_tgt_path\n",
    "        self.src = self.file_reader(src_path)   # 소스와 타겟 파일을 각각 읽어와 리스트로 저장\n",
    "        self.tgt = self.file_reader(tgt_path)\n",
    "        self.data_id = [item.split()[0] for item in self.tgt]   # 각 라인에서 문서 id 추출 -> data_id 리스트로 저장\n",
    "        self.src = [\" \".join(item.split()[1:]) for item in self.src] # 소스와 타겟 데이터에서 각 라인의 첫번째 단어를 제외한\n",
    "        self.tgt = [\" \".join(item.split()[1:]) for item in self.tgt] # 나머지 단어들을 다시 연결하여 문장으로 만듬\n",
    "        # get tokenized test\n",
    "        print('==================== Tokening {} set ======================'.format(mode))\n",
    "        self.src_ids = self.tokenize(self.src)\n",
    "        self.tgt_ids = self.tokenize(self.tgt)\n",
    "\n",
    "    # __len__과 __getitem__ 메소드를 통해 데이터셋의 크기와 인덱스를 통해 데이터를 가져오는 기능\n",
    "    def __len__(self):\n",
    "        return len(self.src)    # 데이터셋의 크기를 반환. self.src의 길이와 같다.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_ids[idx], self.tgt_ids[idx], self.data_id[idx]\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        tokenized_text = [self.tokenizer.encode(i, add_special_tokens=False) for i in tqdm(data)]\n",
    "        return tokenized_text\n",
    "        # tqdm : python에서 진행 상황을 표시하는 라이브러리. 작업의 진행률을 표시해줌\n",
    "\n",
    "    def file_reader(self, file_path):\n",
    "        file = open(file_path, 'r')\n",
    "        lines = [item.strip('\\n') for item in file.readlines()]\n",
    "        return lines\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        if self.args.model == 'text_only_bart':\n",
    "            # rebuild the raw text and truncate to max length\n",
    "            max_input_len = self.args.max_input_len\n",
    "            max_output_len = self.args.max_output_len\n",
    "            raw_src = [pair[0] for pair in data]\n",
    "            raw_tgt = [pair[1] for pair in data]\n",
    "            raw_src = [i[:max_input_len-1] for i in raw_src]\n",
    "            raw_tgt = [i[:max_output_len-1] for i in raw_tgt]\n",
    "            src = []\n",
    "            tgt = []\n",
    "            # remove blank data\n",
    "            for i in range(len(raw_src)):\n",
    "                src.append(raw_src[i])\n",
    "                tgt.append(raw_tgt[i])\n",
    "            # make input mask\n",
    "            mask = torch.tensor(get_mask(src, max_len=max_input_len))\n",
    "            # make input ids\n",
    "            src_ids = torch.tensor(pad_sents(src, 1, max_len=max_input_len)[0])\n",
    "            # make output ids\n",
    "            decoder_ids = [[0]+i for i in tgt]\n",
    "            # make output labels\n",
    "            label_ids = [i+[2] for i in tgt]\n",
    "            decoder_ids = torch.tensor(pad_sents(decoder_ids, 1, max_len=max_output_len)[0])\n",
    "            label_ids = torch.tensor(pad_sents(label_ids, -100, max_len=max_output_len)[0])\n",
    "\n",
    "            return src_ids, decoder_ids, mask, label_ids\n",
    "        \n",
    "        elif self.args.model == 'multi_modal_bart':\n",
    "            # rebuild the raw text and truncate to max length\n",
    "            max_input_len = self.args.max_input_len\n",
    "            max_output_len = self.args.max_output_len\n",
    "            max_img_len = self.args.max_img_len\n",
    "            raw_src = [pair[0] for pair in data]\n",
    "            raw_tgt = [pair[1] for pair in data]\n",
    "            data_id = [pair[2] for pair in data]\n",
    "            raw_src = [i[:max_input_len-1] for i in raw_src]\n",
    "            raw_tgt = [i[:max_output_len-1] for i in raw_tgt]\n",
    "            src = []\n",
    "            tgt = []\n",
    "            img = np.zeros([len(raw_src), self.args.max_img_len, 2048])\n",
    "            img_len = []\n",
    "            # remove blank data\n",
    "            for i in range(len(raw_src)):\n",
    "                src.append(raw_src[i])\n",
    "                tgt.append(raw_tgt[i])\n",
    "                image_feature = np.load(self.args.image_feature_path + data_id[i]+ '.npy')[:max_img_len]\n",
    "                img[i][:image_feature.shape[0]] = image_feature\n",
    "                img_len.append(image_feature.shape[0])\n",
    "            img = img[:,:max(img_len)]\n",
    "\n",
    "            # make input mask\n",
    "            mask = torch.tensor(get_mask(src, max_len=max_input_len))\n",
    "            # make input ids\n",
    "            src_ids = torch.tensor(pad_sents(src, 1, max_len=max_input_len)[0])\n",
    "            # make output ids\n",
    "            decoder_ids = [[0]+i for i in tgt]\n",
    "            # make output labels\n",
    "            label_ids = [i+[2] for i in tgt]\n",
    "            decoder_ids = torch.tensor(pad_sents(decoder_ids, 1, max_len=max_output_len)[0])\n",
    "            label_ids = torch.tensor(pad_sents(label_ids, -100, max_len=max_output_len)[0])\n",
    "            return src_ids, decoder_ids, mask, label_ids, torch.tensor(img), img_len\n",
    "\n",
    "        elif self.args.model == 'text_only_t5':\n",
    "            # rebuild the raw text and truncate to max length\n",
    "            max_input_len = self.args.max_input_len\n",
    "            max_output_len = self.args.max_output_len\n",
    "            raw_src = [pair[0] for pair in data]\n",
    "            raw_tgt = [pair[1] for pair in data]\n",
    "            raw_src = [i[:max_input_len-1] for i in raw_src]\n",
    "            raw_tgt = [i[:max_output_len-1] for i in raw_tgt]\n",
    "            src = []\n",
    "            tgt = []\n",
    "            # remove blank data\n",
    "            for i in range(len(raw_src)):\n",
    "                src.append(raw_src[i])\n",
    "                tgt.append(raw_tgt[i])\n",
    "            # make input mask\n",
    "            mask = torch.tensor(get_mask(src, max_len=max_input_len))\n",
    "            # make input ids\n",
    "            src_ids = torch.tensor(pad_sents(src, 0, max_len=max_input_len)[0])\n",
    "            # make output ids\n",
    "            decoder_ids = [[0]+i for i in tgt]\n",
    "            # make output labels\n",
    "            label_ids = [i+[1] for i in tgt]\n",
    "            decoder_ids = torch.tensor(pad_sents(decoder_ids, 0, max_len=max_output_len)[0])\n",
    "            label_ids = torch.tensor(pad_sents(label_ids, -100, max_len=max_output_len)[0])\n",
    "\n",
    "            return src_ids, decoder_ids, mask, label_ids\n",
    "\n",
    "        elif self.args.model == 'multi_modal_t5':\n",
    "            # rebuild the raw text and truncate to max length\n",
    "            max_input_len = self.args.max_input_len\n",
    "            max_output_len = self.args.max_output_len\n",
    "            max_img_len = self.args.max_img_len\n",
    "            raw_src = [pair[0] for pair in data]\n",
    "            raw_tgt = [pair[1] for pair in data]\n",
    "            data_id = [pair[2] for pair in data]\n",
    "            raw_src = [i[:max_input_len-1] for i in raw_src]\n",
    "            raw_tgt = [i[:max_output_len-1] for i in raw_tgt]\n",
    "            src = []\n",
    "            tgt = []\n",
    "            img = np.zeros([len(raw_src), self.args.max_img_len, 2048])\n",
    "            img_len = []\n",
    "            # remove blank data\n",
    "            for i in range(len(raw_src)):\n",
    "                src.append(raw_src[i])\n",
    "                tgt.append(raw_tgt[i])\n",
    "                if self.args.vision_use_noise:\n",
    "                    image_feature = np.load(self.args.image_feature_path + data_id[i] + '_noise.npy')[:max_img_len]\n",
    "                else:\n",
    "                    image_feature = np.load(self.args.image_feature_path + data_id[i] + '.npy')[:max_img_len]\n",
    "                # image_feature = np.load(self.args.image_feature_path + data_id[i]+ '.npy')[:max_img_len]\n",
    "                img[i][:image_feature.shape[0]] = image_feature\n",
    "                img_len.append(image_feature.shape[0])\n",
    "            img = img[:,:max(img_len)]\n",
    "\n",
    "            # make input mask\n",
    "            mask = torch.tensor(get_mask(src, max_len=max_input_len))\n",
    "            # make input ids\n",
    "            src_ids = torch.tensor(pad_sents(src, 0, max_len=max_input_len)[0])\n",
    "            # make output ids\n",
    "            decoder_ids = [[0]+i for i in tgt]\n",
    "            # make output labels\n",
    "            label_ids = [i+[1] for i in tgt]\n",
    "            decoder_ids = torch.tensor(pad_sents(decoder_ids, 0, max_len=max_output_len)[0])\n",
    "            label_ids = torch.tensor(pad_sents(label_ids, -100, max_len=max_output_len)[0])\n",
    "            return src_ids, decoder_ids, mask, label_ids, torch.tensor(img), img_len\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model\")\n",
    "\n",
    "# Create a dataloading module as per the PyTorch Lightning Docs\n",
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, args):\n",
    "    super().__init__()\n",
    "    train_set = OurDataset(args, 'train')\n",
    "    val_set = OurDataset(args, 'val')\n",
    "    test_set = OurDataset(args, 'test')\n",
    "    self.train_loader = DataLoader(dataset=train_set, \\\n",
    "                                    batch_size=args.batch_size, \\\n",
    "                                    num_workers=3, \\\n",
    "                                    shuffle=True, \\\n",
    "                                    collate_fn=train_set.collate_fn)\n",
    "    self.val_loader = DataLoader(dataset=val_set, \\\n",
    "                                    batch_size=args.batch_size, \\\n",
    "                                    num_workers=3, \\\n",
    "                                    shuffle=False, \\\n",
    "                                    collate_fn=val_set.collate_fn)\n",
    "    self.test_loader = DataLoader(dataset=test_set, \\\n",
    "                                    batch_size=args.batch_size, \\\n",
    "                                    num_workers=3, \\\n",
    "                                    shuffle=False, \\\n",
    "                                    collate_fn=test_set.collate_fn)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return self.train_loader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return self.val_loader\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function function.__dir__()>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OurDataset.collate_fn.__dir__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggplm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
